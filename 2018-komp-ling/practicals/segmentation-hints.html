<!--
SPDX-License-Identifier: (CC-BY-SA-4.0 OR GFDL-1.3-or-later)
Copyright 2018 Nick Howell,
-->
<h1>Practical 1: Segmentation/Tokenization</h1>
<div style="column-width: 30em">
<h2>Comparison of segmenters</h2>
<p>I chose to compare segmenters <code>nltk</code> and <code>pragmatic_segmenter</code>.</p>
<h3><code>pragmatic_segmenter</code></h3>
<p>Create the segmenting program recommended in the practical.</p>
<h3><code>nltk</code></h3>
<p>Here's a sample python program that mimics the behavior of <code>segmenter.rb</code> from the
<code>pragmatic_segmenter</code> example; save as <code>segmenter.py</code> in the same directory as
<code>segmenter.rb</code></p>
<pre><code class="language-python">import sys
from nltk.tokenize import sent_tokenize

for line in sys.stdin:
	for sent in sent_tokenize(line)
		print(sent)

</code></pre>
<h3>Comparison</h3>
<p>You can run each segmenter (remember that the command you enter starts with <code>$</code> and the response has no prefix).</p>
<pre><code class="language-bash">$ echo &quot;This is a test.This is a test. This is a test&quot; | python segmenter.py
This is a test.This is a test.
This is a test.
</code></pre>
<pre><code class="language-bash">$ echo &quot;This is a test.This is a test. This is a test&quot; | ruby -I. segmenter.rb
This is a test.
This is a test.
This is a test.
</code></pre>
<p>Here we see qualitatively that <code>pragmatic_segmenter</code> handles full-stops better
than the <code>nltk</code> segmenter.</p>
<p>For quantitative comparison, a first step is to compare segmentation results:
using the corpus of paragraphs, which I assume is called <code>wiki.txt</code>, you can run each
segmenter and compare them using the tool <code>diff</code>:</p>
<pre><code class="language-bash">python segmenter.py &lt; wiki.txt &gt; segmented-nltk
ruby -I. segmenter.rb &lt; wiki.txt &gt; segmented-pragmatic

diff -U0 segmented-nltk segmented-pragmatic
</code></pre>
<p>Lines starting with <code>+</code> appear only in the second file (<code>segmented-pragmatic</code>)
and with <code>-</code> appear only in the first (<code>segmented-nltk</code>). Thus with our test we
would see:</p>
<pre><code class="language-patch">--- segmented-nltk	2018-11-02 18:53:08.930294409 +0300
+++ segmented-pragmatic	2018-11-02 18:53:18.380294413 +0300
@@ -1 +1,2 @@
-This is a test.This is a test.
+This is a test.
+This is a test.
</code></pre>
<p>This shows us that <code>pragmatic_segmenter</code> split 'This is a test.This is a test.'
(which <code>nltk</code> thought was unsplittable) into 'This is a test.' and 'This is a
test.'</p>
<p>You can write a program which loops over the paragraphs in <code>wiki.txt</code> and runs
the two segmenters on each of them, and compares the output of the two
segmenters, then run statistics: does <code>pragmatic_segmenter</code> produce more or
fewer sentences on average than <code>nltk</code>? Are there cases where it's the other
way around? Is there a paragraph where they completely disagree on sentence
boundaries (i.e., there is no place to split sentences where they agree)?</p>
<h2>Tokenization with maxmatch</h2>
<p>My maxmatch implementation reads sentences from standard input and writes words
to standard output. To extract sentences from a <code>.conllu</code> file, you can search for the comments starting with &quot;&quot;<code># text =</code>&quot;&quot;, then strip off that prefix.</p>
<pre><code class="language-bash">$ grep '^# text = ' ja-test.conllu | sed 's/^# text = //g' &gt; test-sentences
</code></pre>
<p>To extract the correct segmentation, we filter only the columnar data and choose the second column using <code>cut</code>. (The same technique works to create the dictionary file, but remember to discard duplicates using <code>sort -u</code> before saving.)</p>
<pre><code class="language-bash">$ grep '^[0-9]' ja-test.conllu | cut -d'	' -f2 &gt; test-segmented-correct
</code></pre>
<p>Our segmentation we create using our program; I assume you've called it
<code>maxmatch.py</code>:</p>
<pre><code class="language-bash">$ python maxmatch.py dict &lt; test-sentences &gt; test-segmented-maxmatch
</code></pre>
<p>You can then apply exactly the same analysis techniques as for analyzing the
sentence segmenters, using <code>diff</code> or writing a per-sentence program.</p>
</div>
